{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1790c033",
   "metadata": {},
   "source": [
    "## Step 1 --> Installing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec52867",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy pandas sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c555097e",
   "metadata": {},
   "source": [
    "## Step 2 ---> importing necessary files and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f106d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model  import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32827dc1",
   "metadata": {},
   "source": [
    "## Step 3 ----> Now, let’s read the data into a DataFrame(using the pandas library), and get the shape of the data and the first 5 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3694d065",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reading the data\n",
    "data_frame = pd.read_csv('/home/ashish/projects/Fake-NEWS-Detection/news.csv')\n",
    "\n",
    "##Getting the head and shape of the dataframe\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a7927",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of the dataset is:: \",end=\" \")\n",
    "data_frame.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ee9717",
   "metadata": {},
   "source": [
    "## Step 4 ----> Getting the labels from the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba4f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data_frame.label\n",
    "labels.head(n=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6729d1",
   "metadata": {},
   "source": [
    "## Step 5 ---->  Splitting the data into two sets i.e. training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87bc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Splitting the dataset\n",
    "\n",
    "##sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None(it may be any integer between 0 to 42), shuffle=True, stratify=None)\n",
    "x_train,x_test,y_train,y_test = train_test_split(data_frame['text'],labels, test_size=0.2,random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deffe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b038c3f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc137e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0e2563",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f482278",
   "metadata": {},
   "source": [
    "# To Be Remembered ::\n",
    "\n",
    "## 1. class sklearn.feature_extraction.text.TfidfVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer='word', stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.float64'>, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "## 2. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms.\n",
    "\n",
    "## 3. Stop words are words like “and”, “the”, “him”, which are presumed to be uninformative in representing the content of a text, and which may be removed to avoid them being construed as signal for prediction. Sometimes, however, similar words are useful for prediction, such as in classifying writing style or personality. There are several known issues in our provided ‘english’ stop word list.\n",
    "\n",
    "## 4. max_df  :  float or int, default=1.0 ----- >  When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float in range [0.0, 1.0], the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "\n",
    "## 5. fit_transform(raw_documents, y=None  ---->  Learn vocabulary and idf, return document-term matrix.\n",
    "\n",
    "## This is equivalent to fit followed by transform, but more efficiently implemented.\n",
    "\n",
    "# Parameters :\n",
    "## raw_documents : iterable ----> An iterable which generates either str, unicode or file objects.\n",
    "\n",
    "## y : None\n",
    "##     This parameter is ignored.\n",
    "\n",
    "## Returns :  X : sparse matrix of (n_samples, n_features)\n",
    "## Tf-idf-weighted document-term matrix.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 6. TF (Term Frequency): The number of times a word appears in a document is its Term Frequency. A higher value means a term appears more often than others, and so, the document is a good match when the term is part of the search terms.\n",
    "\n",
    "\n",
    "## 7. DF (Inverse Document Frequency): Words that occur many times a document, but also occur many times in many others, may be irrelevant. IDF is a measure of how significant a term is in the entire corpus.\n",
    "\n",
    "# --> The TfidfVectorizer converts a collection of raw documents into a matrix of TF-IDF features\n",
    "\n",
    "\n",
    "## 8. fit_transform(raw_documents, y=None)  ------> Learn vocabulary and idf, return document-term matrix.\n",
    "## This is equivalent to fit followed by transform, but more efficiently implemented.\n",
    "## returns  :  X  :  sparse matrix of (n_samples, n_features)  = Tf-idf-weighted document-term matrix.\n",
    "\n",
    "\n",
    "\n",
    "## 9. transform(raw_documents) -----> Transform documents to document-term matrix.\n",
    "## Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform).\n",
    "## returns  :  X  :  sparse matrix of (n_samples, n_features)  = Tf-idf-weighted document-term matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6016a03",
   "metadata": {},
   "source": [
    "# Step 6 ----> Let’s initialize a TfidfVectorizer with stop words from the English language and a maximum document frequency of 0.7 (terms with a higher document frequency will be discarded). Stop words are the most common words in a language that are to be filtered out before processing the natural language data. \n",
    "\n",
    "## And a TfidfVectorizer turns a collection of raw documents into a matrix of TF-IDF features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ada41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fiting and transforming the vectorizer on the train set, and then transform the vectorizer on the test set.\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words = 'english',max_df=0.7)\n",
    "\n",
    "\n",
    "## Fit and transform the train_set, then transfrom the test_set\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(x_train)\n",
    "tfidf_test = tfidf_vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09459dc1",
   "metadata": {},
   "source": [
    "# To Be Remembered\n",
    "## What is a PassiveAggressiveClassifier?\n",
    "\n",
    "## Passive Aggressive algorithms are online learning algorithms. Such an algorithm remains passive for a correct classification outcome, and turns aggressive in the event of a miscalculation, updating and adjusting. Unlike most other algorithms, it does not converge. Its purpose is to make updates that correct the loss, causing very little change in the norm of the weight vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4777474f",
   "metadata": {},
   "source": [
    "## Step 7 ---> Next, we’ll initialize a PassiveAggressiveClassifier. This is where We’ll fit this PassiveAggressiveClassifier on tfidf_train and y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4bd1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialising a PassiveAggressiveClassifier\n",
    "pac = PassiveAggressiveClassifier(max_iter=50)\n",
    "pac.fit(tfidf_train,y_train)\n",
    "\n",
    "## Making predictions on the test set \n",
    "y_pred = pac.predict(tfidf_test)\n",
    "\n",
    "\n",
    "## Calculating accuracy\n",
    "score  = accuracy_score(y_test,y_pred)\n",
    "print(f'Accuracy is : {round(score*100,2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47e47ec",
   "metadata": {},
   "source": [
    "#                            ACCCURACY = 92.74%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9efc9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "455dbd23",
   "metadata": {},
   "source": [
    "## Step 8 --->   So, We got an accuracy of 92.74% with this model. Finally, let’s print out a confusion matrix to gain insight into the number of false and true negatives and positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad5ef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,y_pred,labels=['FAKE','REAL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b163e8",
   "metadata": {},
   "source": [
    "# INSIGHT/OUTCOME -------> This implies  that in this model, we have 589 true positives, 587 true negatives, 42 false positives, and 49 false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4bb40f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
